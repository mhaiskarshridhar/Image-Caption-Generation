{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP5623_CW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a31sFIyrHaXl",
        "colab_type": "text"
      },
      "source": [
        "# COMP5623 Coursework on Image Caption Generation\n",
        "\n",
        "Starter code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81kdnnwJvTFx",
        "colab_type": "text"
      },
      "source": [
        "## Text preparation \n",
        "\n",
        "We need to build a vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szItYBsE3iVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import re\n",
        "import torch\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXpWOFqFOXcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mounted Drive if using Colab; otherwise, your local path\n",
        "root = \"drive/My Drive/Colab Notebooks/data/Flickr8k/\" # <--- replace this with your root data directory\n",
        "caption_dir = root + \"captions/\"                       # <--- replace these too\n",
        "image_dir = root + \"images/\"                           # <---\n",
        "\n",
        "token_file = \"Flickr8k.token.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9AkORttFoF_",
        "colab_type": "text"
      },
      "source": [
        "A helper function to read in our ground truth text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKNw6OYPA895",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHC0y7zaOXq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filepath):\n",
        "    \"\"\" Open the ground truth captions into memory, line by line. \"\"\"\n",
        "    file = open(filepath, 'r')\n",
        "    lines = []\n",
        "\n",
        "    while True: \n",
        "        # Get next line from file until there's no more\n",
        "        line = file.readline() \n",
        "        if not line: \n",
        "            break\n",
        "        lines.append(line.strip())\n",
        "    file.close() \n",
        "    return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D86cJx2yv81K",
        "colab_type": "text"
      },
      "source": [
        "**You** can read all the ground truth captions (5 per image), into memory as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m-snsM2XHuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_lines(caption_dir + token_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IkK91ZuXNB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines[5:15]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oksUJjLPwApA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
        "    def __init__(self):\n",
        "        # Intially, set both the IDs and words to empty dictionaries.\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # If the word does not already exist in the dictionary, add it\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            # Increment the ID for the next word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        # If we try to access a word in the dictionary which does not exist, return the <unk> id\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEQtthpXwEoY",
        "colab_type": "text"
      },
      "source": [
        "Extract all the words from ```lines```, and create a list of them in a variable ```words```, for example:\n",
        "\n",
        "```words = [\"a\", \"an\", \"the\", \"cat\"... ]```\n",
        "\n",
        "No need to worry about duplicates.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9M3UWSAwAsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word=[]\n",
        "import re\n",
        "import string\n",
        "table = str.maketrans('','', string.punctuation)\n",
        "for i in range(0,len(lines)):\n",
        "  string_to_list=re.split('\\t',lines[i])[1].lower()\n",
        "  new_s = string_to_list.translate(str.maketrans(' ',' ',string.punctuation))  \n",
        "  word.extend(new_s.split())  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHBMe-ATwLIQ",
        "colab_type": "text"
      },
      "source": [
        "Build the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctwErx_ZwAzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a vocab instance\n",
        "vocab = Vocabulary()\n",
        "\n",
        "# Add the token words first\n",
        "vocab.add_word('<pad>')\n",
        "vocab.add_word('<start>')\n",
        "vocab.add_word('<end>')\n",
        "vocab.add_word('<unk>')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzEYIvJ-GA_G",
        "colab_type": "text"
      },
      "source": [
        "Add the rest of the words from the parsed captions:\n",
        "\n",
        "``` vocab.add_word('new_word')```\n",
        "\n",
        "Don't add words that appear three times or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj99JT2XwA4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Exclude words having count less than 3\n",
        "from collections import Counter\n",
        "c=Counter(word)\n",
        "for key,value in c.items():\n",
        "  if value > 3:\n",
        "    vocab.add_word(key)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChA2jcHxX0Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vocab.__len__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB30f4wYwSvg",
        "colab_type": "text"
      },
      "source": [
        "## Dataset and loaders for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raEOHrpnbbKY",
        "colab_type": "text"
      },
      "source": [
        "Keeping the same order, concatenate all the cleaned words from each caption into a string again, and add them all to a list of strings ```cleaned_captions```. Store all the image ids in a list ```image_ids```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGGnaDIRbZUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_captions=[]\n",
        "image_ids=[]\n",
        "#add cleaned captions to cleaned_captions\n",
        "for i in range(0,len(lines)):\n",
        "  string_to_list=re.split('\\t',lines[i])[1].lower()\n",
        "  updatedString=''\n",
        "  for j in string_to_list.split():\n",
        "    updatedString +=vocab.idx2word[vocab.__call__(j)] + ' '\n",
        "  cleaned_captions.append(updatedString)\n",
        "\n",
        "#add image id to image_ids\n",
        "  string_to_list=re.split('\\t',lines[i])[0]\n",
        "  image_ids.append(string_to_list.split('.')[0])\n",
        "cleaned_captions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_FbII1VwVSg",
        "colab_type": "text"
      },
      "source": [
        "The dataframe for the image paths and captions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYQz4T3mwA2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'image_id': image_ids,\n",
        "    'path': [image_dir + image_id + \".jpg\" for image_id in image_ids],\n",
        "    'caption': cleaned_captions\n",
        "}\n",
        "\n",
        "data_df = pd.DataFrame(data, columns=['image_id', 'path', 'caption'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POB7UiJLwYsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(data_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNLQ0K-_weJy",
        "colab_type": "text"
      },
      "source": [
        "This is the Flickr8k class for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqf2_F6YwakD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "from nltk import tokenize\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Flickr8k(Dataset):\n",
        "    \"\"\" Flickr8k custom dataset compatible with torch.utils.data.DataLoader. \"\"\"\n",
        "    \n",
        "    def __init__(self, df, vocab, transform=None):\n",
        "        \"\"\" Set the path for images, captions and vocabulary wrapper.\n",
        "        \n",
        "        Args:\n",
        "            df: df containing image paths and captions.\n",
        "            vocab: vocabulary wrapper.\n",
        "            transform: image transformer.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Returns one data pair (image and caption). \"\"\"\n",
        "\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = self.df['caption'][index]\n",
        "        img_id = self.df['image_id'][index]\n",
        "        path = self.df['path'][index]\n",
        "\n",
        "        image = Image.open(open(path, 'rb'))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert caption (string) to word ids.\n",
        "        tokens = caption.split()\n",
        "        caption = []\n",
        "        # Build the Tensor version of the caption, with token words\n",
        "        caption.append(vocab('<start>'))\n",
        "        caption.extend([vocab(token) for token in tokens])\n",
        "        caption.append(vocab('<end>'))\n",
        "        target = torch.Tensor(caption)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vkld_4CwkPO",
        "colab_type": "text"
      },
      "source": [
        "We need to overwrite the default PyTorch ```collate_fn()``` because our ground truth captions are sequential data of varying lengths. The default ```collate_fn()``` does not support merging the captions with padding.\n",
        "\n",
        "You can read more about it here: https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5YmKr9ewkqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def caption_collate_fn(data):\n",
        "    \"\"\" Creates mini-batch tensors from the list of tuples (image, caption).\n",
        "    Args:\n",
        "        data: list of tuple (image, caption). \n",
        "            - image: torch tensor of shape (3, 256, 256).\n",
        "            - caption: torch tensor of shape (?); variable length.\n",
        "    Returns:\n",
        "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
        "        targets: torch tensor of shape (batch_size, padded_length).\n",
        "        lengths: list; valid length for each padded caption.\n",
        "    \"\"\"\n",
        "    # Sort a data list by caption length from longest to shortest.\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
        "    lengths = [len(cap) for cap in captions]\n",
        "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = cap[:end]        \n",
        "    return images, targets, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6VDx2O5FSiM",
        "colab_type": "text"
      },
      "source": [
        "Now we define the data transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpRbVk6BFTGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Crop size matches the input dimensions expected by the pre-trained ResNet\n",
        "data_transform = transforms.Compose([ \n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),  # Why do we choose 224 x 224?\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),   # Using ImageNet norms\n",
        "                         (0.229, 0.224, 0.225))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgS9OpZ7FaAj",
        "colab_type": "text"
      },
      "source": [
        "Initialising the datasets. The only twist is that every image has 5 \n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "ground truth captions, so each image appears five times in the dataframe. We don't want an image to appear in more than one set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnTvR684GGVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unit_size = 5\n",
        "\n",
        "train_split = 0.95 # Defines the ratio of train/test data.\n",
        "\n",
        "# We didn't shuffle the dataframe yet so this works\n",
        "train_size = unit_size * round(len(data_df)*train_split / unit_size)\n",
        "dataset_train = Flickr8k(\n",
        "    df=data_df[:train_size].reset_index(drop=True),\n",
        "    vocab=vocab,\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "dataset_test = Flickr8k(\n",
        "    df=data_df[(train_size):].reset_index(drop=True),\n",
        "    vocab=vocab,\n",
        "    transform=data_transform,\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWuWg72dGOq9",
        "colab_type": "text"
      },
      "source": [
        "Write the dataloaders ```train_loader``` and ```test_loader``` - explicitly replacing the collate_fn:\n",
        "\n",
        "```train_loader = torch.utils.data.DataLoader(\n",
        "  ...,\n",
        "  collate_fn=caption_collate_fn\n",
        ")```\n",
        "\n",
        "Set train batch size to 128 and be sure to set ```shuffle=True```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkNrIRbXGLFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=caption_collate_fn)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=caption_collate_fn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXlf8lt5TF0N",
        "colab_type": "text"
      },
      "source": [
        "## Encoder and decoder models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls8lyXA2GTC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True) # Pre-trained on ImageNet by default\n",
        "        layers = list(resnet.children())[:-1]      # Keep all layers except the last one\n",
        "        # Unpack the layers and create a new Sequential\n",
        "        self.resnet = nn.Sequential(*layers)\n",
        "        \n",
        "        # We want a specific output size, which is the size of our embedding, so\n",
        "        # we feed our extracted features from the last fc layer (dimensions 1 x 1000)\n",
        "        # into a Linear layer to resize\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        \n",
        "        # Batch normalisation helps to speed up training\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        # What is an embedding layer?\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Define this layer (one at a time)\n",
        "        # self.lstm / self.rnn\n",
        "        #################################################################################################\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True) \n",
        "        # self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True) \n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "    def forward(self, features, captions, lengths):\n",
        "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        # What is \"packing\" a padded sequence?\n",
        "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
        "        # Replace with self.rnn when using RNN #################################################################################################\n",
        "        # hiddens, _ = self.lstm(packed) \n",
        "        hiddens, _ = self.rnn(packed)\n",
        "        outputs = self.linear(hiddens[0])\n",
        "        return outputs\n",
        "    \n",
        "    def sample(self, features, states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        for i in range(self.max_seq_length):\n",
        "            # hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)#########################################\n",
        "            hiddens, states = self.rnn(inputs, states)\n",
        "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
        "            sampled_ids.append(predicted)\n",
        "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9-qtkkMTrtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhecFOMRUgpe",
        "colab_type": "text"
      },
      "source": [
        "Set training parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fd2-IX2Uer3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "log_step = 10\n",
        "save_step = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlIwF6P8UgB4",
        "colab_type": "text"
      },
      "source": [
        "Initialize the models and set the learning parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxwDUlR2Uy7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Build the models\n",
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimisation will be on the parameters of BOTH the enocder and decoder,\n",
        "# but excluding the ResNet parameters, only the new added layers.\n",
        "params = list(\n",
        "    decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters()\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C4lBySBL_XB",
        "colab_type": "text"
      },
      "source": [
        "Function to display captions of test images before and during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NEaXvabA-S5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_words(sentence1):\n",
        "  sent_with_char=[]\n",
        "  char_list=[]\n",
        "  for  char in sentence1.split():\n",
        "    if char=='<start>' or char=='<end>' or char=='<unk>':\n",
        "      None\n",
        "    else:\n",
        "      char_list.append(char)\n",
        "  return char_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oev4fvJJL-Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence(sampled_ids):\n",
        "  # Convert word_ids to words\n",
        "  sampled_caption = []\n",
        "  for word_id in sampled_ids:\n",
        "    word = vocab.idx2word[word_id]\n",
        "    sampled_caption.append(word)\n",
        "    if word == '<end>':\n",
        "      break\n",
        "  sentence = ' '.join(sampled_caption)\n",
        "\n",
        "  return sentence\n",
        "\n",
        "def caption_test_images(images,caption):\n",
        "  # images=data_transform(images)  since the images are already transformed\n",
        "  encoder.eval() \n",
        "  # Generate an caption from the image\n",
        "  images = images.to(device)\n",
        "  caption = caption.to(device)\n",
        "  features = encoder(images)\n",
        "  sampled_ids = decoder.sample(features)\n",
        "  sampled_ids = sampled_ids[0].cpu().numpy()      \n",
        "  sentence_list=sentence_to_words(sentence(sampled_ids))\n",
        "  return sentence_list\n",
        "\n",
        "\n",
        "#Added theis function for generating BLEU score for test dataset since both encoders are different\n",
        "\n",
        "def caption_test_images_testset(images,caption,value):\n",
        "  # images=data_transform(images)  since the images are already transformed\n",
        "  # Generate an caption from the image\n",
        "  images = images.to(device)\n",
        "  caption = caption.to(device)\n",
        "  if value==0:\n",
        "    features = test_encoder_0(images)\n",
        "    sampled_ids = test_decoder_0.sample(features)\n",
        "  if value==1:\n",
        "    features = test_encoder_1(images)\n",
        "    sampled_ids = test_decoder_1.sample(features)\n",
        "  if value==2:\n",
        "    features = test_encoder_2(images)\n",
        "    sampled_ids = test_decoder_2.sample(features)\n",
        "  if value==3:\n",
        "    features = test_encoder_3(images)\n",
        "    sampled_ids = test_decoder_3.sample(features)\n",
        "  if value==4:\n",
        "    features = test_encoder_4(images)\n",
        "    sampled_ids = test_decoder_4.sample(features)\n",
        "\n",
        "  sampled_ids = sampled_ids[0].cpu().numpy()      \n",
        "  sentence_list=sentence_to_words(sentence(sampled_ids))\n",
        "  return sentence_list\n",
        "\n",
        "def reference_caption(test_image):\n",
        "  sentence_list=[]\n",
        "  test_image=test_image.to(device)\n",
        "  for i, (images, captions, lengths) in enumerate(test_loader):\n",
        "    images=images.to(device)\n",
        "    if torch.equal(test_image, images) == True:\n",
        "      sentence1=sentence(captions.cpu().detach().numpy()[0])\n",
        "      sentence_list.append(sentence_to_words(sentence1))\n",
        "  return sentence_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDlCcKOlMjPg",
        "colab_type": "text"
      },
      "source": [
        "Test Images\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T16tJZkMl70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "dataiter = iter(test_loader)\n",
        "test_image1, test_captions1, test_lengths1 = dataiter.next()\n",
        "test_image1_display=test_image1[0]\n",
        "test_image2, test_captions2, test_lengths2 = dataiter.next()\n",
        "test_image2_display=test_image2[0]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRDYDW2jOtda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_image(display_image):\n",
        "    tensor_image = display_image + 1\n",
        "    tensor_image = tensor_image - tensor_image.min()\n",
        "    picture = tensor_image / (tensor_image.max() - tensor_image.min())\n",
        "\n",
        "    plt.imshow(picture.permute(1, 2, 0))\n",
        "    plt.axis(\"off\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoWO7CxBQmuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Display 1st Image\n",
        "display_image(test_image1_display)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDt04CyoQ1cE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Display 2nd Image\n",
        "display_image(test_image2_display)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUmSb2MHEZw3",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2yHJrhPAm-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_set():\n",
        "  score_sum=0\n",
        "  print('In test set')\n",
        "  for i, (images, captions, lengths) in enumerate(test_loader):\n",
        "    reference_caption_test=reference_caption(images)\n",
        "    generated_caption_test=caption_test_images(images,captions)\n",
        "    score = sentence_bleu(reference_caption_test, generated_caption_test,weights=(0.25, 0.25,0.25, 0.25))\n",
        "    score_sum=score_sum+score\n",
        "  score_sum=score_sum/len(test_loader)\n",
        "  return score_sum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS4oN21vNKu7",
        "colab_type": "text"
      },
      "source": [
        "The loop to train the model. Feel free to put this in a function if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M7KY9G3NI8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the models\n",
        "\n",
        "#Reference Caption\n",
        "reference_caption_1=reference_caption(test_image1)\n",
        "print(' Reference Captions before training for Image 1')\n",
        "for i in reference_caption_1:\n",
        "  print(i)\n",
        "reference_caption_2=reference_caption(test_image2)\n",
        "print('Reference Caption before training for Image 2')\n",
        "for i in reference_caption_2:\n",
        "  print(i)\n",
        "#Generated Caption\n",
        "generated_caption_1=caption_test_images(test_image1, test_captions1)\n",
        "print('Genarated caption for Image 1 before training',generated_caption_1)\n",
        "generated_caption_2=caption_test_images(test_image2, test_captions2)\n",
        "print('Genarated caption for Image 2 before training',generated_caption_2)\n",
        "\n",
        "#BLEU Score Image 1\n",
        "score1= sentence_bleu(reference_caption_1, generated_caption_1,weights=(0.25, 0.25,0.25, 0.25))\n",
        "print('BLEU score for Image 1 before traing',score1)\n",
        "\n",
        "#BLEU Score Image 2\n",
        "score2 = sentence_bleu(reference_caption_2, generated_caption_2,weights=(0.25, 0.25,0.25, 0.25))\n",
        "print('BLEU score for Image 2 before traing',score2)\n",
        "\n",
        "#TRAIN MDOEL\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    loss_sum=0\n",
        "    for i, (images, captions, lengths) in enumerate(train_loader):\n",
        "\n",
        "        # Set mini-batch dataset\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        # Packed as well as we'll compare to the decoder outputs\n",
        "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
        "\n",
        "        # Forward, backward and optimize\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions, lengths)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # Zero gradients for both networks\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print log info\n",
        "        if i % log_step == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch, num_epochs, i, total_step, loss.item())) \n",
        "        loss_sum=loss_sum+loss.item()\n",
        "    loss_sum=loss_sum/301\n",
        "    print('\\n')\n",
        "    print('Epoch:',epoch,' Loss',loss_sum)\n",
        "\n",
        "    print('Caption after epoch',epoch)\n",
        "    generated_caption_1=caption_test_images(test_image1, test_captions1)\n",
        "    generated_caption_2=caption_test_images(test_image2, test_captions2)\n",
        "    score1 = sentence_bleu(reference_caption_1, generated_caption_1,weights=(0.25, 0.25,0.25, 0.25))\n",
        "    score2 = sentence_bleu(reference_caption_2, generated_caption_2,weights=(0.25, 0.25,0.25, 0.25))\n",
        "    print('Genarated caption for Image 1',generated_caption_1)\n",
        "    print('Genarated caption for Image 2',generated_caption_2)\n",
        "    print('BLEU score for Image 1',score1)\n",
        "    print('BLEU score for Image 2',score2)\n",
        "    print('\\n')\n",
        "    print(' For epoch: ',epoch,' BLEU score for whole test dataset',test_set())\n",
        "    \n",
        "    # If you want to save the model checkpoints - recommended once you have everything working\n",
        "    # Make sure to save RNN and LSTM versions separately\n",
        "    torch.save(decoder.state_dict(), 'drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/decoder_train_exp1_rnn-epoch{}.pth'.format(epoch))\n",
        "    torch.save(encoder.state_dict(), 'drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/encoder_train_exp1_rnn-epoch{}.pth'.format(epoch))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnlU4iSwAlhN",
        "colab_type": "text"
      },
      "source": [
        "Calculating BLEU score for entire dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7-f9hzAkwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "test_encoder_0 = EncoderCNN(embed_size).to(device)\n",
        "test_decoder_0 = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "blue_scores_0 = []\n",
        "checkpoint_decoder_0 = torch.load('drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/decoder_train_exp1_rnn-epoch0.pth') \n",
        "checkpoint_encoder_0 = torch.load('drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/encoder_train_exp1_rnn-epoch0.pth') \n",
        "\n",
        "test_encoder_1 = EncoderCNN(embed_size).to(device)\n",
        "test_decoder_1 = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "blue_scores_1 = []\n",
        "checkpoint_decoder_1 = torch.load('drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/decoder_train_exp1_rnn-epoch1.pth') \n",
        "checkpoint_encoder_1 = torch.load('drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/encoder_train_exp1_rnn-epoch1.pth') \n",
        "\n",
        "test_encoder_2 = EncoderCNN(embed_size).to(device)\n",
        "test_decoder_2 = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "blue_scores_2 = []\n",
        "checkpoint_decoder_2 = torch.load('drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/decoder_train_exp1_rnn-epoch2.pth') \n",
        "checkpoint_encoder_2 = torch.load('drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/encoder_train_exp1_rnn-epoch2.pth') \n",
        "\n",
        "test_encoder_3 = EncoderCNN(embed_size).to(device)\n",
        "test_decoder_3 = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "blue_scores_3 = []\n",
        "checkpoint_decoder_3 = torch.load('drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/decoder_train_exp1_rnn-epoch3.pth') \n",
        "checkpoint_encoder_3 = torch.load('drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/encoder_train_exp1_rnn-epoch3.pth') \n",
        "\n",
        "test_encoder_4 = EncoderCNN(embed_size).to(device)\n",
        "test_decoder_4 = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "blue_scores_4 = []\n",
        "checkpoint_decoder_4 = torch.load('drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/decoder_train_exp1_rnn-epoch4.pth') \n",
        "checkpoint_encoder_4 = torch.load('drive/My Drive/Colab Notebooks/data/Flickr8k/model_path/encoder_train_exp1_rnn-epoch4.pth') \n",
        "\n",
        "#file_encoder = '/content/drive/My Drive/Colab Notebooks/RNN/encoder1.ckpt'\n",
        "# NOW LOAD THE CHECKPOINTS TO THE MODELS\n",
        "test_encoder_0.load_state_dict(checkpoint_encoder_0)\n",
        "test_decoder_0.load_state_dict(checkpoint_decoder_0)\n",
        "\n",
        "test_encoder_1.load_state_dict(checkpoint_encoder_1)\n",
        "test_decoder_1.load_state_dict(checkpoint_decoder_1)\n",
        "\n",
        "test_encoder_2.load_state_dict(checkpoint_encoder_2)\n",
        "test_decoder_2.load_state_dict(checkpoint_decoder_2)\n",
        "\n",
        "test_encoder_3.load_state_dict(checkpoint_encoder_3)\n",
        "test_decoder_3.load_state_dict(checkpoint_decoder_3)\n",
        "\n",
        "test_encoder_4.load_state_dict(checkpoint_encoder_4)\n",
        "test_decoder_4.load_state_dict(checkpoint_decoder_4)\n",
        "\n",
        "# SETTING THE MODELS TO EVAL MODE\n",
        "test_encoder_0.eval()\n",
        "test_decoder_0.eval()\n",
        "\n",
        "test_encoder_1.eval()\n",
        "test_decoder_1.eval()\n",
        "\n",
        "test_encoder_2.eval()\n",
        "test_decoder_2.eval()\n",
        "\n",
        "test_encoder_3.eval()\n",
        "test_decoder_3.eval()\n",
        "\n",
        "test_encoder_4.eval()\n",
        "test_decoder_4.eval()\n",
        "\n",
        "for i, (images, targets, lengths) in enumerate(test_loader): # Iterating the test data loader\n",
        "  print(i)\n",
        "  reference_caption_test=reference_caption(images)\n",
        "\n",
        "  generated_caption_test=caption_test_images_testset(images,targets,0)\n",
        "  bleu_score = sentence_bleu(reference_caption_test, generated_caption_test,weights=(0.25, 0.25,0.25, 0.25))\n",
        "  print('BLEU Score 0',bleu_score)\n",
        "  blue_scores_0.append(bleu_score)\n",
        "\n",
        "  generated_caption_test=caption_test_images_testset(images,targets,1)\n",
        "  bleu_score = sentence_bleu(reference_caption_test, generated_caption_test,weights=(0.25, 0.25,0.25, 0.25))\n",
        "  print('BLEU Score 1',bleu_score)\n",
        "  blue_scores_1.append(bleu_score)\n",
        "\n",
        "  generated_caption_test=caption_test_images_testset(images,targets,2)\n",
        "  bleu_score = sentence_bleu(reference_caption_test, generated_caption_test,weights=(0.25, 0.25,0.25, 0.25))\n",
        "  print('BLEU Score 2',bleu_score)\n",
        "  blue_scores_2.append(bleu_score)\n",
        "\n",
        "  generated_caption_test=caption_test_images_testset(images,targets,3)\n",
        "  bleu_score = sentence_bleu(reference_caption_test, generated_caption_test,weights=(0.25, 0.25,0.25, 0.25))\n",
        "  print('BLEU Score3',bleu_score)\n",
        "  blue_scores_3.append(bleu_score)\n",
        "  \n",
        "  generated_caption_test=caption_test_images_testset(images,targets,4)\n",
        "  bleu_score = sentence_bleu(reference_caption_test, generated_caption_test,weights=(0.25, 0.25,0.25, 0.25))\n",
        "  print('BLEU Score 4',bleu_score)\n",
        "  blue_scores_4.append(bleu_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5peCtV2VRKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sum(blue_scores_0)/len(blue_scores_0))\n",
        "print(sum(blue_scores_1)/len(blue_scores_1))\n",
        "print(sum(blue_scores_2)/len(blue_scores_2))\n",
        "print(sum(blue_scores_3)/len(blue_scores_3))\n",
        "print(sum(blue_scores_4)/len(blue_scores_4))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}